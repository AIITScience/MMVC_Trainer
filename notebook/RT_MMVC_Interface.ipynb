{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# モデルの精度を確認するためのインターフェース\n",
        "\n",
        "ver.2022/03/19"
      ],
      "metadata": {
        "id": "U7-O9IRzogIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 概要\n",
        "「Train_RT-MMVC.ipynb」で学習したモデルでTTSと非リアルタイムのVCを行い、モデルの精度を検証します。"
      ],
      "metadata": {
        "id": "dK2UHlpmoyLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Google Drive をマウント\n",
        "**Google Drive にアップロードした RT-MMVC_Trainer を参照できるように、設定します。**\n",
        "\n",
        "「このノートブックに Google ドライブのファイルへのアクセスを許可しますか？」\n",
        "\n",
        "といったポップアップが表示されるので、「Google ドライブに接続」を押下し、google アカウントを選択して、「許可」を選択してください。\n",
        "\n",
        "成功すれば、下記メッセージが出ます。\n",
        "```\n",
        "Mounted at /content/drive/\n",
        "```\n"
      ],
      "metadata": {
        "id": "uRK7k2o7pL5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s8Ozg6regVi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cdコマンドを実行して、マウントしたGoogle Drive のRT-MMVC_Trainerディレクトリに移動します。\n",
        "\n",
        "%cd 「RT-MMVC_Trainerをgoogle driveにパップロードしたパス」\n",
        "\n",
        "としてください。\n",
        "\n",
        "正しいパスが指定されていれば\n",
        "\n",
        "-rw------- 1 root root 11780 Mar  4 16:53 attentions.py\n",
        "\n",
        "-rw------- 1 root root  4778 Mar  4 16:53 commons.py\n",
        "\n",
        "drwx------ 2 root root  4096 Mar  5 15:20 configs\n",
        "\n",
        "...といった感じに表示されるはずです。"
      ],
      "metadata": {
        "id": "6mD03f7LpR97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaNipgu-enJo"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 必要なライブラリのインストール\n",
        "\n",
        "何も考えず実行してください。"
      ],
      "metadata": {
        "id": "2CP8NcKZpZNA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOWo15aRewCk"
      },
      "outputs": [],
      "source": [
        "!apt-get install espeak\n",
        "!pip install Cython==0.29.21\n",
        "!pip install librosa==0.8.0\n",
        "!pip install matplotlib==3.3.1\n",
        "!pip install numpy==1.18.5\n",
        "!pip install phonemizer==2.2.1\n",
        "!pip install scipy==1.5.2\n",
        "!pip install tensorboard==2.3.0\n",
        "!pip install torch==1.6.0\n",
        "!pip install torchvision==0.7.0\n",
        "!pip install Unidecode==1.1.1\n",
        "!pip install retry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h03ETxHHDkQ2"
      },
      "outputs": [],
      "source": [
        "!pip install pyopenjtalk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3QYLvY4e38A"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import commons\n",
        "import utils\n",
        "from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
        "from models import SynthesizerTrn\n",
        "from text.symbols import symbols\n",
        "from text import text_to_sequence\n",
        "\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "\n",
        "def get_text(text, hps):\n",
        "    text_norm = text_to_sequence(text, hps.data.text_cleaners)\n",
        "    if hps.data.add_blank:\n",
        "        text_norm = commons.intersperse(text_norm, 0)\n",
        "    text_norm = torch.LongTensor(text_norm)\n",
        "    return text_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PDT7lfLDjNY"
      },
      "outputs": [],
      "source": [
        "import pyopenjtalk\n",
        "def mozi2phone(mozi):\n",
        "    text = pyopenjtalk.g2p(mozi)\n",
        "    text = \"sil \" + text + \" sil\"\n",
        "    text = text.replace(' ', '-')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 学習したモデルを読み込む"
      ],
      "metadata": {
        "id": "eAdIexFDoeym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFIG_PATH に学習に利用したjsonファイルを`「./configs/****.json」`のように指定し、\n",
        "\n",
        "NET_PATHに学習したモデルを`「./configs/xxxx/G_*****.pth」`のように指定してください。"
      ],
      "metadata": {
        "id": "faOeDsFXpql9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_PATH = \"./configs/baseconfig_seg4192.json\"\n",
        "NET_PATH = \"./logs/20220306_24000/G_348000.pth\""
      ],
      "metadata": {
        "id": "Rm-3oWmarsZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "指定したファイルをもとにモデルの読み込みを行います。"
      ],
      "metadata": {
        "id": "9EAsizUNsGAw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecUDV8_ee8OP"
      },
      "outputs": [],
      "source": [
        "hps = utils.get_hparams_from_file(CONFIG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYrcO66SfCqD"
      },
      "outputs": [],
      "source": [
        "net_g = SynthesizerTrn(\n",
        "    len(symbols),\n",
        "    hps.data.filter_length // 2 + 1,\n",
        "    hps.train.segment_size // hps.data.hop_length,\n",
        "    n_speakers=hps.data.n_speakers,\n",
        "    **hps.model)\n",
        "_ = net_g.eval()\n",
        "\n",
        "_ = utils.load_checkpoint(NET_PATH, net_g, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 学習したモデルでTTSを行う"
      ],
      "metadata": {
        "id": "H6XCTRpgsTII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEXTに**ひらがな**で文章を入力してください。"
      ],
      "metadata": {
        "id": "rL1wGpP8te8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = \"おはよう、しょくん。わたしはてんのうずあいるだ。これはおんせいごうせいのてすとだ。どうだ？すごいだろう。\""
      ],
      "metadata": {
        "id": "kIy1xAiItn7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TTS_Speaker_IDにTTSを実行する話者IDを代入してください。  \n",
        "(filelists/xxxx_Correspondence.txtに話者Idと対応した話者の対応が出力されています！)"
      ],
      "metadata": {
        "id": "dQuihULMdjX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TTS_Speaker_ID = 0"
      ],
      "metadata": {
        "id": "Sx0_TsvKd8Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際にTTSを行います。\n",
        "\n",
        "イントネーションがおかしくなることが多々ありますが、VC的には問題なので気にしなくて大丈夫です。\n",
        "\n",
        "初回のみpyopenjtalk周りのダウンロードが始まるので、少し時間がかかりますが仕様です。"
      ],
      "metadata": {
        "id": "fILJqQ6pt5bX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXeNhEV7fI36"
      },
      "outputs": [],
      "source": [
        "text = TEXT\n",
        "stn_tst = get_text(mozi2phone(text), hps)\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_tst = stn_tst.unsqueeze(0)\n",
        "    x_tst_lengths = torch.LongTensor([stn_tst.size(0)])\n",
        "    sid = torch.LongTensor([TTS_Speaker_ID])\n",
        "    audio = net_g.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.400, noise_scale_w=0.8, length_scale=1)[0][0,0].data.cpu().float().numpy()\n",
        "ipd.display(ipd.Audio(audio, rate=hps.data.sampling_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 学習したモデルで非リアルタイムVCを行う"
      ],
      "metadata": {
        "id": "m-ho5133vpFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非リアルタイムのVCを行います。\n",
        "\n",
        "ソース話者のIDとその話者の音声ファイルのパス、変換ターゲットの話者のIDを指定してください。"
      ],
      "metadata": {
        "id": "uRtrz7GIwyGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_WAVFILE = \"dataset/textful/000_jvs001/wav/VOICEACTRESS100_001.wav\"\n",
        "SOURCE_SPEAKER_ID = 0\n",
        "TARGET_ID = 1"
      ],
      "metadata": {
        "id": "uEqm8yA6v9xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際にVCを行います。\n",
        "\n",
        "ここでの性能が悪い場合、学習不足か他に問題があります。"
      ],
      "metadata": {
        "id": "NHy-FQAYxLOR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vkotLtNY_s4"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    dataset = TextAudioSpeakerLoader(hps.data.validation_files_notext, hps.data)\n",
        "    data = dataset.get_audio_text_speaker_pair([SOURCE_WAVFILE, SOURCE_SPEAKER_ID, \"a\"])\n",
        "    data = TextAudioSpeakerCollate()([data])\n",
        "    x, x_lengths, spec, spec_lengths, y, y_lengths, sid_src = [x for x in data]\n",
        "    sid_tgt1 = torch.LongTensor([TARGET_ID])\n",
        "    audio1 = net_g.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt1)[0][0,0].data.cpu().float().numpy()\n",
        "print(\"Original SID: %d\" % sid_src.item())\n",
        "ipd.display(ipd.Audio(y[0].cpu().numpy(), rate=hps.data.sampling_rate))\n",
        "print(\"Converted SID: %d\" % sid_tgt1.item())\n",
        "ipd.display(ipd.Audio(audio1, rate=hps.data.sampling_rate))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "RT-MMVC_Interface_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}